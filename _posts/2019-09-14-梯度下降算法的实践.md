---
layout: post
title: "梯度下降算法实践"
date: 2019-09-14
tag: 机器学习
---

### 梯度下降算法数学理论基础



<img src="https://upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip|imageView2/2/w/1047/format/webp" style="zoom: 50%;" />

> 梯度前加一个负号，就意味着朝着梯度相反的方向前进！梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号。

这里需要定义一个代价函数：

<img src="https://upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png" alt="代价函数" style="zoom:67%;" />

这里需要我们需要拟合的直线形式是：

<img src="https://upload-images.jianshu.io/upload_images/1234352-acea37db1e02004d.png?imageMogr2/auto-orient/strip|imageView2/2/w/328/format/webp" alt="直线形式" style="zoom:67%;" />

注： 因为theta0无参数，所以为了计算，我们需要在x上增加一维，来更好的计算

我们需要的是，通过公式不断批量更新theta，然后求出最合适的theta0，和theta1

<img src="https://upload-images.jianshu.io/upload_images/1234352-bfd1c5136eaaa552.png?imageMogr2/auto-orient/strip|imageView2/2/w/485/format/webp" alt="theta0" style="zoom: 67%;" />

我们把代价函数转化为我们计算机易于理解的形式----矩阵形式

<img src="https://upload-images.jianshu.io/upload_images/1234352-66b04086dd1f8ba9.png?imageMogr2/auto-orient/strip|imageView2/2/w/516/format/webp" style="zoom:67%;" />

下面是梯度下降的代码:

```python
import numpy as np
import matplotlib.pyplot as plt

def gradient_function(X, theta, y):   # 这里实质上是求导，梯度的进一步求导
    diff = np.dot(X, theta) - y
    return (1/m)*np.dot(X.T, diff) 


def DesentProcess(X, y, rate):
    theta = np.array([1, 1]).reshape(2, 1)  # 初始theta0和theta1为1，1
    gradient = gradient_function(X, theta, y)
    while np.all(np.absolute(gradient) > 1e-5):
        theta = theta - rate*gradient
        gradient = gradient_function(X, theta, y)
    return theta


x_values = [1, 2, 3, 0.6, 3.6, 1.5, 3.1, 2.4, 1.8, 2.5, 2.6, 0.3, 1.5, 1.65, 2.6, 4.2, 1.5, 0.5, 2.13]
y_values = [0.3, 1.5, 2.2, 0.2, 3.2, 0.5, 2.8, 1.3, 1.5, 3, 3, 2.0, 2, 1.9, 3, 5, 3, 1, 4]

plt.scatter(x_values, y_values, s=100, color="blue")  # 散点图
m = 19
X0 = np.ones((m, 1))  # 需要增加一个维度，这里是为了更好的对theta0进行处理
X1 = np.array([x_values]).reshape(m, 1)  # 塑造一个m*1的矩阵
X = np.hstack((X0, X1))  # 两个矩阵进行左右合并
y = np.array([y_values]).reshape(m, 1)
learnRate = 0.01   # 学习率
D = DesentProcess(X, y, learnRate)   # 这里得出theta0和theta1的结果
# 下面是画直线的过程
x = np.arange(0, 5, 0.1)             
z = D[0] + D[1] * x    
plt.plot(x, z)
plt.show()
print(D)

```

![拟合的直线](\images\posts\others\recrusion.png)

转载请注明：[瞿晶的博客](http://fantongxue.xyz) » [点击阅读原文](http://www.fantongxue.xyz/2019/09/梯度下降算法的实践/)